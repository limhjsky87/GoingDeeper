{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flexible-dressing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94773248/94765736 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "personalized-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deconv module\n",
    "upconv1 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn1 = tf.keras.layers.BatchNormalization()\n",
    "relu1 = tf.keras.layers.ReLU()\n",
    "upconv2 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn2 = tf.keras.layers.BatchNormalization()\n",
    "relu2 = tf.keras.layers.ReLU()\n",
    "upconv3 = tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same')\n",
    "bn3 = tf.keras.layers.BatchNormalization()\n",
    "relu3 = tf.keras.layers.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "maritime-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = tf.keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "upconv = _make_deconv_layer(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cardiac-cocktail",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = tf.keras.layers.Conv2D(17, kernel_size=(1,1), padding='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comparative-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 256, 192, 3)]     0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, None, None, 2048)  23587712  \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 64, 48, 256)       10489600  \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 48, 1)         257       \n",
      "=================================================================\n",
      "Total params: 34,077,569\n",
      "Trainable params: 34,022,913\n",
      "Non-trainable params: 54,656\n",
      "_________________________________________________________________\n",
      "(1, 256, 192, 3)\n",
      "(1, 64, 48, 1)\n",
      "tf.Tensor(\n",
      "[[[ 0.0005819 ]\n",
      "  [-0.00031189]\n",
      "  [-0.02409783]\n",
      "  [-0.01189323]\n",
      "  [-0.01007731]\n",
      "  [ 0.01841793]\n",
      "  [-0.03823639]\n",
      "  [-0.00464023]\n",
      "  [-0.00489871]\n",
      "  [ 0.00405288]]\n",
      "\n",
      " [[ 0.00663753]\n",
      "  [-0.02913981]\n",
      "  [-0.02737622]\n",
      "  [-0.07418466]\n",
      "  [-0.02927101]\n",
      "  [-0.02753114]\n",
      "  [-0.01461075]\n",
      "  [-0.06312663]\n",
      "  [-0.02090156]\n",
      "  [-0.04264103]]\n",
      "\n",
      " [[-0.01143088]\n",
      "  [-0.00155626]\n",
      "  [-0.04107168]\n",
      "  [-0.01457007]\n",
      "  [-0.04712829]\n",
      "  [-0.01922984]\n",
      "  [-0.03260932]\n",
      "  [-0.0571216 ]\n",
      "  [-0.06581447]\n",
      "  [-0.01505517]]\n",
      "\n",
      " [[-0.00625249]\n",
      "  [-0.05333596]\n",
      "  [-0.02279449]\n",
      "  [-0.06017819]\n",
      "  [-0.02951106]\n",
      "  [-0.06533568]\n",
      "  [-0.02763944]\n",
      "  [-0.02183751]\n",
      "  [-0.0560061 ]\n",
      "  [-0.06096578]]\n",
      "\n",
      " [[-0.02585564]\n",
      "  [-0.03930252]\n",
      "  [-0.03667884]\n",
      "  [-0.03808434]\n",
      "  [ 0.00120005]\n",
      "  [-0.01269854]\n",
      "  [-0.08516636]\n",
      "  [-0.0481335 ]\n",
      "  [-0.03287528]\n",
      "  [-0.03494755]]\n",
      "\n",
      " [[-0.00029796]\n",
      "  [-0.01796702]\n",
      "  [-0.04488219]\n",
      "  [-0.05036775]\n",
      "  [-0.03197875]\n",
      "  [-0.0468423 ]\n",
      "  [-0.0494772 ]\n",
      "  [-0.08150841]\n",
      "  [-0.03570705]\n",
      "  [-0.06448731]]\n",
      "\n",
      " [[-0.02047599]\n",
      "  [-0.02682519]\n",
      "  [-0.01868036]\n",
      "  [-0.02295383]\n",
      "  [-0.04609685]\n",
      "  [-0.05701993]\n",
      "  [-0.06267509]\n",
      "  [-0.07776572]\n",
      "  [-0.10052711]\n",
      "  [-0.06864805]]\n",
      "\n",
      " [[-0.01510959]\n",
      "  [-0.02900411]\n",
      "  [ 0.01500863]\n",
      "  [-0.00870089]\n",
      "  [-0.05013182]\n",
      "  [-0.06327307]\n",
      "  [-0.06864419]\n",
      "  [ 0.0040316 ]\n",
      "  [-0.11326557]\n",
      "  [-0.04394741]]\n",
      "\n",
      " [[-0.01914253]\n",
      "  [-0.01385871]\n",
      "  [-0.03483472]\n",
      "  [-0.0639713 ]\n",
      "  [-0.0400741 ]\n",
      "  [ 0.00811424]\n",
      "  [-0.05385813]\n",
      "  [-0.06702783]\n",
      "  [-0.07687519]\n",
      "  [-0.09418175]]\n",
      "\n",
      " [[-0.00062652]\n",
      "  [-0.05638286]\n",
      "  [-0.02132822]\n",
      "  [-0.07235243]\n",
      "  [-0.05383978]\n",
      "  [-0.06368028]\n",
      "  [-0.03358739]\n",
      "  [-0.07096133]\n",
      "  [-0.075216  ]\n",
      "  [-0.10553274]]], shape=(10, 10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def _make_deconv_layer(num_deconv_layers):\n",
    "    seq_model = keras.models.Sequential()\n",
    "    for i in range(num_deconv_layers):\n",
    "        seq_model.add(tf.keras.layers.Conv2DTranspose(256, kernel_size=(4,4), strides=(2,2), padding='same'))\n",
    "        seq_model.add(tf.keras.layers.BatchNormalization())\n",
    "        seq_model.add(tf.keras.layers.ReLU())\n",
    "    return seq_model\n",
    "\n",
    "resnet = tf.keras.applications.resnet.ResNet50(include_top=False, weights='imagenet')\n",
    "upconv = _make_deconv_layer(3)\n",
    "final_layer = tf.keras.layers.Conv2D(1, kernel_size=(1,1), padding='same')\n",
    "\n",
    "# input :  192x256\n",
    "# output : 48x64\n",
    "inputs = keras.Input(shape=(256, 192, 3))\n",
    "x = resnet(inputs)\n",
    "x = upconv(x)\n",
    "out = final_layer(x)\n",
    "model = keras.Model(inputs, out)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "np_input = np.random.randn(1,256,192,3)\n",
    "np_input = np.zeros((1,256,192,3), dtype=np.float32)\n",
    "tf_input = tf.convert_to_tensor(np_input, dtype=np.float32)\n",
    "print (tf_input.shape) # TensorShape([1,256,192,3])\n",
    "\n",
    "tf_output = model(tf_input)\n",
    "\n",
    "print (tf_output.shape)\n",
    "print (tf_output[0,:10,:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "human keypoint detection\n",
    "HP\n",
    "\n",
    "Top-down\n",
    "모든 사람의 정확한 keypoint 를 찾기 위해 object detection 을 사용\n",
    "crop 한 이미지 내에서 keypoint 를 찾아내는 방법으로 표현\n",
    "detector가 선행되어야 하고 모든 사람마다 알고리즘을 적용해야하기 때문에 사람이 많이 등장할 때는 느리다는 단점이 있다.\n",
    "Bottom-up\n",
    "detector가 없고 keypoint 를 먼저 검출 예로 손목에 해당하는 모든 점들을 검출\n",
    "한 사람에 해당하는 keypoint 를 clustering\n",
    "detector 가 없기 때문에 다수의 사람이 영상에 등장하더라도 속도 저하가 크지 않다.\n",
    "top down 방식에 비해 keypoint 검출범위가 넓어 성능이 떨어진다는 단점이 있다.\n",
    "Convolutional Pose Machines\n",
    "CVPR 2016에서 발표된 CPM 은 completely differentiable 한 multi-stage 구조를 제안했습니다. multi stage 방법들은 DeepPose 에서부터 지속적으로 사용되어 왔었습니다. 하지만 crop 연산 등 비연속적인 미분불가능한 stage 단위로 나눠져 있었기 때문에 학습 과정을 여러번 반복하는 비효율적인 방법을 사용해왔습니다.\n",
    "\n",
    "Convolutional Pose Machines\n",
    "CPM 은 end-to-end 로 학습할 수 있는 모델을 제안합니다.1\n",
    "Stage 1 은 image feature 를 계산하는 역할을 하고 stage 2는 keypoint 를 예측하는 역할을 합니다. g1과 g2 모두 heatmap 을 출력하게 만들어서 재사용이 가능한 부분은 weight sharing 할 수 있도록 세부 모델을 설계 했습니다.\n",
    "\n",
    "2\n",
    "\n",
    "Stage ≥ 2 에서 볼 수 있듯이 stage 2 이상부터는 반복적으로 사용할 수 있습니다. 보통은 3개의 스테이지를 사용한다고 합니다. stage 1 구조는 고정이고 stage 2 부터는 stage 2 구조를 반복해서 추론합니다. stage 2 부터는 입력이 heatmap(image feature)이 되기 때문에 stage 단계를 거칠수록 keypoint가 refinement 되는 효과를 볼 수 있습니다.3\n",
    "사실 CPM 이 아주 좋은 방법이라고는 말하기 어렵습니다. Multi-stage 방법을 사용하기 때문에 end-to-end 로 학습이 가능하더라도 그대로 학습하는 경우는 높은 성능을 달성하기 어렵습니다. 따라서 stage 단위로 pretraining 을 한 후 다시 하나의 모델로 합쳐서 학습을 합니다. 논문을 작성하기 위해서라면 충분히 감내할 수 있지만 서비스 측면에서 바라본다면 불편한 요소라고 할 수 있습니다. 이런 문제점들은 후에 제안되는 모델들이 적극적으로 개선하고 있습니다.\n",
    "\n",
    "CPM 을 다루는 이유는 성능 때문입니다. receptive field 를 넓게 만드는 multi stage refinement 방법이 성능향상에 크게 기여한 것 같습니다.\n",
    "\n",
    "4주황색 실선이 Tompson 알고리즘입니다. CPM 에서 제안한 검정색, 회색 실선이 detection rate에서 유의미한 차이를 보이고 있는 것을 볼 수 있습니다. MPII 의 PCKh@0.5 에서 87.95% 를 달성했다고 합니다. 당시 2등보다 6.11%p 높은 성능을 보였습니다.\n",
    "\n",
    "Stacked Hourglass Network\n",
    "Stacked Hourglass Networks for Human Pose Estimation\n",
    "Stacked Hourglass Network 의 기본 구조는 모래시계 같은 모양으로 만들어져 있습니다. Conv layer 와 pooling 으로 이미지(또는 feature) 를 인코딩 하고 upsampling layer 를 통해 feature map 의 크기를 키우는 방향으로 decoding 합니다. feature map 크기가 작아졌다 커지는 구조여서 hourglass 라고 표현합니다.\n",
    "\n",
    "shn\n",
    "\n",
    "기존 방법들과의 가장 큰 차이점은\n",
    "\n",
    "feature map upsampling residual connection 이라고 할 수 있을 것 같습니다.\n",
    "\n",
    "pooling으로 image의 global feature를 찾고 upsampling으로 local feature를 고려하는 아이디어가 hourglass의 핵심 novelty라고 할 수 있습니다.\n",
    "\n",
    "비교\n",
    "비교\n",
    "(a) : Hourglass (b) : CPN(cascaded pyramid networks) (c) : SimpleBaseline - transposed conv (d) : SimpleBaseline - dilated conv\n",
    "\n",
    "Simplebaseline 이 다른 알고리즘들에 비해 성능이 떨어지지 않지만 구조를 보면 공통점과 차이점이 있는데 무엇일까\n",
    "공통점 : high resolution → low resolution 인 encoder 와 low → high 인 decoder 구조로 이루어진 점\n",
    "\n",
    "차이점 : Hourglass 는 encoder 와 decoder 의 비율이 거의 비슷함(대칭적임). 반면 Simplebaseline 은 encoder 가 무겁고 (resnet50 등 backbone 사용) decoder 는 가벼운 모델을 사용함. (a), (b) 는 skip connection 이 있지만 (c) 는 skip connection 이 없다.\n",
    "\n",
    "기존 모델들은 skip connection 을 적극적으로 사용했는데 왜 사용했을까\n",
    "pooling(strided conv) 할 때 소실되는 정보를 high level layer에서 사용해서 detail한 정보를 학습하기 위해 사용합니다. 당연히 사용할 때 성능이 더 좋을 것 같습니다.\n",
    "\n",
    "HR-NET\n",
    "hrnet\n",
    "\n",
    "HRNet 또한 이전 알고리즘 들과 마찬가지로 heatmap을 regression하는 방식으로 학습하고 MSE loss를 이용합니다. (특히 Simplebaseline 과 거의 유사합니다.)\n",
    "HR-NET github\n",
    "SimpleBaseline 코드구조\n",
    "sb\n",
    "\n",
    "encoder : conv layers\n",
    "decoder : deconv module + upsampling\n",
    "Simple Baselines for Human Pose Estimation and Tracking\n",
    "conv model로 resnet을 사용\n",
    "deconv module은 deconv-bn-relu 이 단계가 3개의 레이어로 이루어져있고, deconv는 256 filter size, 4x4 kernel, stride 2 로 2배씩 feature map이 커집니다.\n",
    "마지막 출력 레이어는 k 개의 1x1 conv layer로 구성\n",
    "microsoft/human-pose-estimation.pytorch\n",
    "\n",
    "nn. 표현이 많이 등장합니다. torch.nn 으로 keras.layers 와 같이 딥러닝 모델 구성에 필요한 도구들이 정의되어 있습니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L29\n",
    "\n",
    "29번째 줄에서는 BasicBlock 이라는 클래스가 보이네요. keras.models 로 model 을 선언하는 것과 비슷합니다.\n",
    "\n",
    "참고로 pytorch model 에서는 사용된 layer 를 forward 함수를 통해 computational graph 를 그려줍니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L42\n",
    "\n",
    "forward 함수를 읽어볼까요? 앗.. 어딘가 많이 본 구조 아닌가요?\n",
    "\n",
    "    residual = x\n",
    "\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = self.relu(out)\n",
    "\n",
    "    out = self.conv2(out)\n",
    "    out = self.bn2(out)\n",
    "\n",
    "    if self.downsample is not None:\n",
    "        residual = self.downsample(x)\n",
    "\n",
    "    out += residual\n",
    "    out = self.relu(out)\n",
    "맞습니다. residual block 을 사용했네요.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L157\n",
    "\n",
    "Pose 메인 model 을 살펴보니 4개의 residual block 을 이용합니다. 완전 resnet 과 동일하죠?\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L234\n",
    "\n",
    "forward 함수를 보면 흐름을 쉽게 알 수 있습니다.\n",
    "\n",
    "def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu(x) x = self.maxpool(x)\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "\n",
    "    x = self.deconv_layers(x)\n",
    "    x = self.final_layer(x)\n",
    "\n",
    "    return x\n",
    "resnet 을 통과한 후 deconv_layers 와 final_layer를 차례로 통과합니다.\n",
    "\n",
    "deconv layer 를 찾아보니,\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/lib/models/pose_resnet.py#L219\n",
    "\n",
    "    layers.append(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels=self.inplanes,\n",
    "                out_channels=planes,\n",
    "                kernel_size=kernel,\n",
    "                stride=2,\n",
    "                padding=padding,\n",
    "                output_padding=output_padding,\n",
    "                bias=self.deconv_with_bias))\n",
    "        layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "transpose conv 와 bn, relu 로 이루어져있는 것을 확인했습니다.\n",
    "\n",
    "세세한 파라미터는 어디에 있을까요?\n",
    "\n",
    "EXTRA 가 자주 등장하는 것을 볼 때, 어떤 configuration 파일이 있을 것으로 짐작해 볼 수 있겠네요. repo 내에서 검색해보면 파라미터 관련 정보를 담고 있는 아래 파일을 찾을 수 있습니다.\n",
    "\n",
    "https://github.com/microsoft/human-pose-estimation.pytorch/blob/master/experiments/coco/resnet50/256x192d256x3adam_lr1e-3.yaml#L23\n",
    "\n",
    "NUM_DECONV_LAYERS: 3 NUM_DECONV_FILTERS:\n",
    "\n",
    "- 256\n",
    "- 256\n",
    "- 256\n",
    "NUM_DECONV_KERNELS:\n",
    "- 4\n",
    "- 4\n",
    "- 4\n",
    "deconv layer 의 파라미터가 아주 상세히 적혀있네요!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
